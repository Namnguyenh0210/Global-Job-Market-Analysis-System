"""
ETL Script - Transform & Clean Jobs Data
X·ª≠ l√Ω d·ªØ li·ªáu t·ª´ raw JSON th√†nh dataset s·∫°ch ƒë·ªÉ ph√¢n t√≠ch
"""

import json
import pandas as pd
from pathlib import Path
import re
from datetime import datetime

# ============================================================================
# C·∫§U H√åNH
# ============================================================================
RAW_DATA_DIR = Path(__file__).parent.parent / 'data' / 'raw_jobs'
OUTPUT_DIR = Path(__file__).parent.parent / 'data'

# Mapping qu·ªëc gia -> khu v·ª±c
COUNTRY_TO_REGION = {
    'sg': 'Southeast Asia',
    'us': 'North America',
    'gb': 'Europe',
    'de': 'Europe',
    'in': 'Asia',
    'it': 'Europe',
    'nl': 'Europe',
    'nz': 'Oceania'
}

# Danh s√°ch k·ªπ nƒÉng c·∫ßn ph√¢n t√≠ch
SKILLS_TO_TRACK = ['Python', 'SQL', 'AWS', 'Excel', 'English']


# ============================================================================
# H√ÄM X·ª¨ L√ù D·ªÆ LI·ªÜU
# ============================================================================

def load_raw_json_files():
    """ƒê·ªçc t·∫•t c·∫£ file JSON t·ª´ th∆∞ m·ª•c raw_jobs"""
    print("\nüìÇ ƒêang ƒë·ªçc raw JSON files...")
    
    all_jobs = []
    json_files = list(RAW_DATA_DIR.glob('*.json'))
    
    if not json_files:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file JSON n√†o trong {RAW_DATA_DIR}")
        print("‚ö†Ô∏è  Vui l√≤ng ch·∫°y extract_jobs.py tr∆∞·ªõc!")
        return []
    
    for json_file in json_files:
        print(f"   üìÑ ƒê·ªçc: {json_file.name}")
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            jobs = data.get('jobs', [])
            country_code = data.get('country_code', '')
            
            # Th√™m country_code v√†o m·ªói job
            for job in jobs:
                job['_country_code'] = country_code
            
            all_jobs.extend(jobs)
    
    print(f"‚úÖ ƒê√£ load {len(all_jobs)} jobs t·ª´ {len(json_files)} files\n")
    return all_jobs


def extract_fields(job):
    """
    Tr√≠ch xu·∫•t c√°c tr∆∞·ªùng c·∫ßn thi·∫øt t·ª´ raw job data
    
    Args:
        job: Dict ch·ª©a th√¥ng tin job t·ª´ API
        
    Returns:
        Dict v·ªõi c√°c tr∆∞·ªùng ƒë√£ chu·∫©n h√≥a
    """
    # Tr√≠ch xu·∫•t salary info
    salary_min = job.get('salary_min')
    salary_max = job.get('salary_max')
    
    # Location
    location = job.get('location', {})
    city = location.get('display_name', '') if isinstance(location, dict) else str(location)
    
    # Company
    company = job.get('company', {})
    company_name = company.get('display_name', 'Unknown') if isinstance(company, dict) else str(company)
    
    return {
        'job_title': job.get('title', ''),
        'company': company_name,
        'country': job.get('_country_code', '').upper(),
        'city': city,
        'salary_min': salary_min,
        'salary_max': salary_max,
        'salary_currency': 'USD',  # Adzuna tr·∫£ v·ªÅ USD m·∫∑c ƒë·ªãnh
        'salary_period': 'year',
        'job_description': job.get('description', ''),
        'date_posted': job.get('created', ''),
        'source': 'Adzuna'
    }


def clean_data(df):
    """
    L√†m s·∫°ch d·ªØ li·ªáu
    
    Args:
        df: DataFrame c·∫ßn l√†m s·∫°ch
        
    Returns:
        DataFrame ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch
    """
    print("üßπ ƒêang l√†m s·∫°ch d·ªØ li·ªáu...")
    
    initial_count = len(df)
    
    # 1. X√≥a duplicates (d·ª±a tr√™n job_title + company)
    df = df.drop_duplicates(subset=['job_title', 'company'], keep='first')
    print(f"   ‚úÖ X√≥a {initial_count - len(df)} jobs tr√πng l·∫∑p")
    
    # 2. G√°n region d·ª±a tr√™n country
    df['region'] = df['country'].str.lower().map(COUNTRY_TO_REGION)
    df['region'] = df['region'].fillna('Other')
    print(f"   ‚úÖ ƒê√£ g√°n region cho t·∫•t c·∫£ jobs")
    
    # 3. X·ª≠ l√Ω missing values
    df['city'] = df['city'].fillna('Unknown')
    df['company'] = df['company'].fillna('Unknown Company')
    df['job_description'] = df['job_description'].fillna('')
    
    # Clean HTML tags t·ª´ description
    df['job_description'] = df['job_description'].apply(clean_html)
    
    # 4. Chu·∫©n h√≥a salary
    # N·∫øu c√≥ salary_min ho·∫∑c salary_max, ƒë√°nh d·∫•u has_salary = True
    df['has_salary'] = (df['salary_min'].notna()) | (df['salary_max'].notna())
    
    print(f"   ‚úÖ ƒê√£ x·ª≠ l√Ω missing values\n")
    
    return df


def clean_html(text):
    """X√≥a HTML tags kh·ªèi text"""
    if not isinstance(text, str):
        return ''
    # Remove HTML tags
    clean_text = re.sub(r'<[^>]+>', '', text)
    # Remove extra whitespace
    clean_text = ' '.join(clean_text.split())
    return clean_text


def analyze_skills(df):
    """
    Ph√¢n t√≠ch k·ªπ nƒÉng ƒë∆∞·ª£c y√™u c·∫ßu trong job descriptions
    
    Args:
        df: DataFrame ch·ª©a jobs
        
    Returns:
        DataFrame v·ªõi c√°c c·ªôt skill m·ªõi (True/False)
    """
    print("üîç ƒêang ph√¢n t√≠ch k·ªπ nƒÉng...")
    
    for skill in SKILLS_TO_TRACK:
        # T√¨m skill trong job_description (case-insensitive)
        pattern = r'\b' + re.escape(skill) + r'\b'
        df[f'skill_{skill.lower()}'] = df['job_description'].str.contains(
            pattern, case=False, regex=True, na=False
        )
        
        count = df[f'skill_{skill.lower()}'].sum()
        percentage = (count / len(df) * 100) if len(df) > 0 else 0
        print(f"   {skill}: {count} jobs ({percentage:.1f}%)")
    
    print()
    return df


def calculate_kpis(df):
    """T√≠nh to√°n c√°c KPIs"""
    print("üìä KPI T·ªïng quan:")
    print(f"   ‚Ä¢ T·ªïng s·ªë jobs: {len(df)}")
    print(f"   ‚Ä¢ S·ªë qu·ªëc gia: {df['country'].nunique()}")
    print(f"   ‚Ä¢ S·ªë c√¥ng ty: {df['company'].nunique()}")
    
    salary_percentage = (df['has_salary'].sum() / len(df) * 100) if len(df) > 0 else 0
    print(f"   ‚Ä¢ Jobs c√≥ l∆∞∆°ng: {df['has_salary'].sum()} ({salary_percentage:.1f}%)")
    print()


def save_output(df):
    """L∆∞u k·∫øt qu·∫£ ra CSV v√† Excel"""
    print("üíæ ƒêang l∆∞u k·∫øt qu·∫£...")
    
    # Ensure output directory exists
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # Save CSV
    csv_file = OUTPUT_DIR / 'clean_jobs.csv'
    df.to_csv(csv_file, index=False, encoding='utf-8')
    print(f"   ‚úÖ ƒê√£ l∆∞u CSV: {csv_file.name}")
    
    # Save Excel
    excel_file = OUTPUT_DIR / 'clean_jobs.xlsx'
    df.to_excel(excel_file, index=False, engine='openpyxl')
    print(f"   ‚úÖ ƒê√£ l∆∞u Excel: {excel_file.name}")
    
    print(f"\nüìÅ Output t·∫°i: {OUTPUT_DIR}")


def main():
    """H√†m main - Transform & Clean data"""
    print("\n" + "="*70)
    print("üöÄ B·∫ÆT ƒê·∫¶U TRANSFORM & CLEAN DATA")
    print("="*70)
    
    # 1. Load raw JSON
    all_jobs = load_raw_json_files()
    
    if not all_jobs:
        print("‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω!")
        return
    
    # 2. Extract fields
    print("üìã ƒêang tr√≠ch xu·∫•t c√°c tr∆∞·ªùng d·ªØ li·ªáu...")
    extracted_data = [extract_fields(job) for job in all_jobs]
    df = pd.DataFrame(extracted_data)
    print(f"‚úÖ ƒê√£ tr√≠ch xu·∫•t {len(df)} jobs\n")
    
    # 3. Clean data
    df = clean_data(df)
    
    # 4. Analyze skills
    df = analyze_skills(df)
    
    # 5. Calculate KPIs
    calculate_kpis(df)
    
    # 6. Save output
    save_output(df)
    
    print("\n" + "="*70)
    print("‚úÖ HO√ÄN TH√ÄNH TRANSFORM & CLEAN!")
    print("="*70)
    print("\nüéØ B∆∞·ªõc ti·∫øp theo: Ch·∫°y FastAPI backend")
    print("   cd backend")
    print("   uvicorn api.main:app --reload")


if __name__ == "__main__":
    main()
